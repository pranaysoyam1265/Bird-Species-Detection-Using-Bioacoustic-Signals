"""
Script: train_fast_quality.py
Purpose: FAST training with FULL QUALITY (no compromises!)
Location: 09_Utils/Scripts/train_fast_quality.py

- Uses pre-computed spectrograms (5-10x faster)
- Full SpecAugment (frequency + time masking)
- Full Mixup augmentation
- Same model architecture
- Same regularization (dropout, weight decay, label smoothing)
- Same learning rate schedule

QUALITY = IDENTICAL TO SLOW VERSION
SPEED = 5-10x FASTER
"""

import os
import gc
import json
import random
import numpy as np
import pandas as pd
from tqdm import tqdm
from datetime import datetime
import time

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler

import timm
import warnings
warnings.filterwarnings('ignore')


# ============================================================
# CONFIGURATION
# ============================================================
class Config:
    BASE_DIR = r"C:\Users\prana\OneDrive\Desktop\ML Conf-BioFSL"
    
    # Pre-computed spectrograms
    SPEC_DIR = os.path.join(BASE_DIR, "03_Features", "Spectrograms_Precomputed")
    
    # Data splits
    TRAIN_CSV = os.path.join(BASE_DIR, "04_Labels", "Train_Val_Test_Split_Fixed", "train.csv")
    VAL_CSV = os.path.join(BASE_DIR, "04_Labels", "Train_Val_Test_Split_Fixed", "val.csv")
    TEST_CSV = os.path.join(BASE_DIR, "04_Labels", "Train_Val_Test_Split_Fixed", "test.csv")
    LABEL_MAPPING = os.path.join(BASE_DIR, "04_Labels", "Processed_Labels", "label_mapping_fixed.json")
    
    # Output
    SAVE_DIR = os.path.join(BASE_DIR, "05_Model", "Saved_Models")
    LOG_DIR = os.path.join(BASE_DIR, "05_Model", "Training_Logs", f"fast_quality_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    
    # Model (SAME as before)
    MODEL_NAME = "efficientnet_b0"
    NUM_CLASSES = 54
    PRETRAINED = True
    
    # Training hyperparameters (SAME as before)
    EPOCHS = 50
    BATCH_SIZE = 32          # Can use larger batch with pre-computed specs
    LEARNING_RATE = 0.0003
    WEIGHT_DECAY = 0.05
    LABEL_SMOOTHING = 0.1
    PATIENCE = 15
    
    # Data loading (FAST settings)
    NUM_WORKERS = 4
    PIN_MEMORY = True
    PREFETCH_FACTOR = 2
    
    # Augmentation (FULL QUALITY)
    # SpecAugment parameters
    SPEC_AUG_PROB = 0.5
    FREQ_MASK_PARAM = 20      # Max frequency bands to mask
    TIME_MASK_PARAM = 30      # Max time steps to mask
    N_FREQ_MASKS = 2          # Number of frequency masks
    N_TIME_MASKS = 2          # Number of time masks
    
    # Mixup
    USE_MIXUP = True
    MIXUP_ALPHA = 0.4
    MIXUP_PROB = 0.5
    
    # Additional augmentation on spectrograms
    USE_TIME_WARP = True      # Slight time warping
    USE_FREQ_WARP = True      # Slight frequency warping
    
    # Class balancing for handling imbalanced species
    USE_CLASS_WEIGHTS = True
    USE_WEIGHTED_SAMPLER = True
    
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    SEED = 42


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True  # Faster convolutions


def compute_class_weights(train_csv, label_mapping_path, num_classes=54):
    """
    Compute inverse frequency weights for each class.
    Helps with imbalanced species in training data.
    """
    df = pd.read_csv(train_csv)
    class_counts = df['species'].value_counts()
    
    # Load label mapping
    with open(label_mapping_path, 'r') as f:
        label_data = json.load(f)
    
    # Handle both formats: nested dict or flat dict
    if isinstance(list(label_data.values())[0], dict):
        species_to_id = label_data.get('species_to_id', {})
    else:
        species_to_id = label_data
    
    # Create weights array
    weights = torch.zeros(num_classes)
    total_samples = len(df)
    
    for species, idx in species_to_id.items():
        count = class_counts.get(species, 1)
        # Inverse frequency weighting
        weights[idx] = total_samples / (num_classes * count)
    
    # Normalize weights
    weights = weights / weights.sum() * num_classes
    
    return weights


# ============================================================
# SPECTROGRAM AUGMENTATION (Full Quality)
# ============================================================
class SpecAugment(nn.Module):
    """
    Full SpecAugment implementation.
    Paper: https://arxiv.org/abs/1904.08779
    
    Includes:
    - Frequency masking (multiple masks)
    - Time masking (multiple masks)
    - Optional time/frequency warping
    """
    
    def __init__(self, config):
        super().__init__()
        self.freq_mask_param = config.FREQ_MASK_PARAM
        self.time_mask_param = config.TIME_MASK_PARAM
        self.n_freq_masks = config.N_FREQ_MASKS
        self.n_time_masks = config.N_TIME_MASKS
        self.p = config.SPEC_AUG_PROB
        self.use_time_warp = config.USE_TIME_WARP
        self.use_freq_warp = config.USE_FREQ_WARP
    
    def forward(self, spec):
        """
        Apply SpecAugment to spectrogram.
        Input: (C, H, W) tensor where H=n_mels, W=time_frames
        """
        if not self.training or random.random() > self.p:
            return spec
        
        spec = spec.clone()
        _, n_mels, n_frames = spec.shape
        
        # Time warping (subtle, preserves structure)
        if self.use_time_warp and random.random() < 0.3:
            spec = self._time_warp(spec, W=5)
        
        # Frequency masking
        for _ in range(self.n_freq_masks):
            f = random.randint(0, min(self.freq_mask_param, n_mels - 1))
            f0 = random.randint(0, n_mels - f)
            spec[:, f0:f0 + f, :] = 0
        
        # Time masking
        for _ in range(self.n_time_masks):
            t = random.randint(0, min(self.time_mask_param, n_frames - 1))
            t0 = random.randint(0, n_frames - t)
            spec[:, :, t0:t0 + t] = 0
        
        return spec
    
    def _time_warp(self, spec, W=5):
        """Simple time warping by rolling"""
        shift = random.randint(-W, W)
        return torch.roll(spec, shifts=shift, dims=2)


class SpectrogramTransform:
    """
    Additional spectrogram transformations for data augmentation.
    All operations preserve audio characteristics.
    """
    
    def __init__(self, p=0.3):
        self.p = p
    
    def __call__(self, spec):
        """Apply random transformations"""
        if random.random() > self.p:
            return spec
        
        # Random selection of augmentations
        augmentations = []
        
        if random.random() < 0.5:
            augmentations.append(self._random_gain)
        if random.random() < 0.3:
            augmentations.append(self._add_noise)
        if random.random() < 0.3:
            augmentations.append(self._time_shift)
        
        for aug in augmentations:
            spec = aug(spec)
        
        return spec
    
    def _random_gain(self, spec):
        """Random gain adjustment (simulates volume change)"""
        gain = random.uniform(0.8, 1.2)
        return torch.clamp(spec * gain, 0, 1)
    
    def _add_noise(self, spec):
        """Add small amount of noise"""
        noise = torch.randn_like(spec) * 0.01
        return torch.clamp(spec + noise, 0, 1)
    
    def _time_shift(self, spec):
        """Shift in time (simulates different recording start)"""
        shift = random.randint(-10, 10)
        return torch.roll(spec, shifts=shift, dims=2)


# ============================================================
# DATASET
# ============================================================
class FastBirdDataset(Dataset):
    """
    Fast dataset using pre-computed spectrograms.
    Applies full augmentation pipeline.
    """
    
    def __init__(self, csv_path, spec_dir, split_name, label_mapping_path, 
                 config, augment=False):
        self.df = pd.read_csv(csv_path)
        self.spec_dir = os.path.join(spec_dir, split_name)
        self.config = config
        self.augment = augment
        
        # Load label mapping
        with open(label_mapping_path, 'r') as f:
            label_data = json.load(f)
        
        # Handle nested structure
        if isinstance(list(label_data.values())[0], dict):
            self.label_map = label_data.get('species_to_id', {})
        else:
            self.label_map = label_data
        
        # Augmentation modules
        if augment:
            self.spec_aug = SpecAugment(config)
            self.spec_transform = SpectrogramTransform(p=0.3)
        else:
            self.spec_aug = None
            self.spec_transform = None
        
        # Verify spectrograms exist
        self._verify_spectrograms()
        
        print(f"   ðŸ“‚ {split_name}: {len(self.df)} samples, Augment={augment}")
    
    def _verify_spectrograms(self):
        """Verify at least one spectrogram exists"""
        sample = self.df.iloc[0]['chunk_file']
        sample_path = os.path.join(
            self.spec_dir, 
            sample.replace('.wav', '.npy').replace('.mp3', '.npy')
        )
        if not os.path.exists(sample_path):
            raise FileNotFoundError(
                f"Pre-computed spectrograms not found!\n"
                f"Expected: {sample_path}\n"
                f"Run: python 09_Utils/Scripts/precompute_spectrograms.py"
            )
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        filename = row['chunk_file']
        species = row['species']
        
        # Load pre-computed spectrogram
        spec_filename = filename.replace('.wav', '.npy').replace('.mp3', '.npy')
        spec_path = os.path.join(self.spec_dir, spec_filename)
        
        try:
            spec = np.load(spec_path)
        except:
            # Fallback: zero spectrogram
            spec = np.zeros((128, 216), dtype=np.float32)
        
        # Convert to 3-channel tensor
        spec_3ch = np.stack([spec, spec, spec], axis=0).astype(np.float32)
        spec_tensor = torch.from_numpy(spec_3ch)
        
        # Apply augmentations (training only)
        if self.augment:
            # Additional spectrogram transforms
            if self.spec_transform:
                spec_tensor = self.spec_transform(spec_tensor)
            
            # SpecAugment (frequency/time masking)
            if self.spec_aug:
                self.spec_aug.training = True
                spec_tensor = self.spec_aug(spec_tensor)
        
        # Get label
        label = self.label_map.get(species, 0)
        
        return spec_tensor, label


# ============================================================
# MODEL (Same architecture)
# ============================================================
class BirdClassifier(nn.Module):
    """
    EfficientNet-B0 classifier.
    Same architecture as before, no changes.
    """
    
    def __init__(self, config):
        super().__init__()
        
        # Backbone with regularization
        self.backbone = timm.create_model(
            config.MODEL_NAME,
            pretrained=config.PRETRAINED,
            num_classes=0,
            drop_rate=0.3,
            drop_path_rate=0.2,
        )
        
        # Get feature dimension
        with torch.no_grad():
            dummy = torch.zeros(1, 3, 128, 216)
            feat_dim = self.backbone(dummy).shape[1]
        
        # Classifier head with strong regularization
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(feat_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, config.NUM_CLASSES)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        features = self.backbone(x)
        return self.classifier(features)


# ============================================================
# MIXUP (Full implementation)
# ============================================================
def mixup_data(x, y, alpha=0.4):
    """
    Mixup: Creates mixed samples and labels.
    Paper: https://arxiv.org/abs/1710.09412
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1.0
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixup loss computation"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


# ============================================================
# TRAINING FUNCTIONS
# ============================================================
def train_epoch(model, loader, criterion, optimizer, scaler, config, epoch):
    model.train()
    
    total_loss = 0
    correct = 0
    total = 0
    
    pbar = tqdm(loader, desc=f"Train E{epoch}", ncols=110)
    
    for batch_idx, (data, target) in enumerate(pbar):
        data = data.to(config.DEVICE, non_blocking=True)
        target = target.to(config.DEVICE, non_blocking=True)
        
        optimizer.zero_grad(set_to_none=True)
        
        # Apply Mixup with probability
        use_mixup = config.USE_MIXUP and random.random() < config.MIXUP_PROB
        
        if use_mixup:
            data, y_a, y_b, lam = mixup_data(data, target, config.MIXUP_ALPHA)
            
            with autocast():
                output = model(data)
                loss = mixup_criterion(criterion, output, y_a, y_b, lam)
            
            # Accuracy calculation for mixup
            _, pred = output.max(1)
            correct += (lam * pred.eq(y_a).sum().float() + 
                       (1 - lam) * pred.eq(y_b).sum().float()).item()
        else:
            with autocast():
                output = model(data)
                loss = criterion(output, target)
            
            _, pred = output.max(1)
            correct += pred.eq(target).sum().item()
        
        # Backward pass
        scaler.scale(loss).backward()
        
        # Gradient clipping
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item()
        total += target.size(0)
        
        # Update progress bar
        pbar.set_postfix({
            'loss': f'{total_loss/(batch_idx+1):.3f}',
            'acc': f'{100*correct/total:.1f}%'
        })
    
    return total_loss / len(loader), 100 * correct / total


def validate(model, loader, criterion, config, epoch):
    model.eval()
    
    total_loss = 0
    correct = 0
    top3_correct = 0
    top5_correct = 0
    total = 0
    
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        pbar = tqdm(loader, desc=f"Val   E{epoch}", ncols=110)
        
        for data, target in pbar:
            data = data.to(config.DEVICE, non_blocking=True)
            target = target.to(config.DEVICE, non_blocking=True)
            
            with autocast():
                output = model(data)
                loss = criterion(output, target)
            
            total_loss += loss.item()
            
            # Top-1 accuracy
            _, pred = output.max(1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # Top-3 and Top-5 accuracy
            _, top3_pred = output.topk(3, dim=1)
            _, top5_pred = output.topk(5, dim=1)
            
            for i in range(target.size(0)):
                if target[i] in top3_pred[i]:
                    top3_correct += 1
                if target[i] in top5_pred[i]:
                    top5_correct += 1
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            
            pbar.set_postfix({
                'loss': f'{total_loss/len(pbar):.3f}',
                'acc': f'{100*correct/total:.1f}%',
                'top5': f'{100*top5_correct/total:.1f}%'
            })
    
    return {
        'loss': total_loss / len(loader),
        'acc': 100 * correct / total,
        'top3_acc': 100 * top3_correct / total,
        'top5_acc': 100 * top5_correct / total,
        'predictions': all_preds,
        'targets': all_targets
    }


# ============================================================
# MAIN TRAINING LOOP
# ============================================================
def main():
    print("=" * 70)
    print("âš¡ FAST TRAINING WITH FULL QUALITY")
    print("=" * 70)
    
    config = Config()
    set_seed(config.SEED)
    
    # Create directories
    os.makedirs(config.SAVE_DIR, exist_ok=True)
    os.makedirs(config.LOG_DIR, exist_ok=True)
    
    # Print configuration
    print(f"\nðŸ“‹ Configuration:")
    print(f"   Model:           {config.MODEL_NAME}")
    print(f"   Batch Size:      {config.BATCH_SIZE}")
    print(f"   Learning Rate:   {config.LEARNING_RATE}")
    print(f"   Weight Decay:    {config.WEIGHT_DECAY}")
    print(f"   Label Smoothing: {config.LABEL_SMOOTHING}")
    print(f"   Mixup Alpha:     {config.MIXUP_ALPHA}")
    print(f"   SpecAugment:     Freq={config.FREQ_MASK_PARAM}, Time={config.TIME_MASK_PARAM}")
    print(f"   Device:          {config.DEVICE}")
    print(f"   Workers:         {config.NUM_WORKERS}")
    
    # Verify pre-computed spectrograms exist
    if not os.path.exists(config.SPEC_DIR):
        print(f"\nâŒ Pre-computed spectrograms not found!")
        print(f"   Run first: python 09_Utils/Scripts/precompute_spectrograms.py")
        return
    
    # Load datasets
    print(f"\nðŸ“‚ Loading datasets...")
    
    train_ds = FastBirdDataset(
        config.TRAIN_CSV, config.SPEC_DIR, "train",
        config.LABEL_MAPPING, config, augment=True
    )
    
    val_ds = FastBirdDataset(
        config.VAL_CSV, config.SPEC_DIR, "val",
        config.LABEL_MAPPING, config, augment=False
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_ds,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        prefetch_factor=config.PREFETCH_FACTOR,
        persistent_workers=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_ds,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        prefetch_factor=config.PREFETCH_FACTOR,
        persistent_workers=True
    )
    
    print(f"   Train: {len(train_ds)} samples, {len(train_loader)} batches")
    print(f"   Val:   {len(val_ds)} samples, {len(val_loader)} batches")
    
    # Build model
    print(f"\nðŸ—ï¸  Building model...")
    model = BirdClassifier(config).to(config.DEVICE)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   Total params:     {total_params:,}")
    print(f"   Trainable params: {trainable_params:,}")
    
    # Loss function with label smoothing and class weights
    if config.USE_CLASS_WEIGHTS:
        print(f"\nâš–ï¸  Computing class weights for imbalanced species...")
        class_weights = compute_class_weights(config.TRAIN_CSV, config.LABEL_MAPPING, config.NUM_CLASSES)
        class_weights = class_weights.to(config.DEVICE)
        criterion = nn.CrossEntropyLoss(
            weight=class_weights,
            label_smoothing=config.LABEL_SMOOTHING
        )
        print(f"   Class weights computed. Min: {class_weights.min():.4f}, Max: {class_weights.max():.4f}")
    else:
        criterion = nn.CrossEntropyLoss(label_smoothing=config.LABEL_SMOOTHING)
    
    # Optimizer
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY,
        betas=(0.9, 0.999)
    )
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=10,
        T_mult=2,
        eta_min=1e-6
    )
    
    # Gradient scaler for mixed precision
    scaler = GradScaler()
    
    # Training tracking
    best_val_acc = 0
    best_top5_acc = 0
    patience_counter = 0
    
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': [], 'val_top3': [], 'val_top5': [],
        'learning_rate': [], 'epoch_time': []
    }
    
    # Training loop
    print("\n" + "=" * 70)
    print("ðŸš€ TRAINING STARTED")
    print("=" * 70)
    
    total_start_time = time.time()
    
    for epoch in range(1, config.EPOCHS + 1):
        epoch_start_time = time.time()
        
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\n{'â”€' * 70}")
        print(f"Epoch {epoch}/{config.EPOCHS} | LR: {current_lr:.6f}")
        print(f"{'â”€' * 70}")
        
        # Train
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, scaler, config, epoch
        )
        
        # Validate
        val_results = validate(model, val_loader, criterion, config, epoch)
        
        # Update scheduler
        scheduler.step()
        
        # Calculate epoch time
        epoch_time = time.time() - epoch_start_time
        
        # Save history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_results['loss'])
        history['val_acc'].append(val_results['acc'])
        history['val_top3'].append(val_results['top3_acc'])
        history['val_top5'].append(val_results['top5_acc'])
        history['learning_rate'].append(current_lr)
        history['epoch_time'].append(epoch_time)
        
        # Print results
        gap = train_acc - val_results['acc']
        print(f"\nðŸ“Š Results:")
        print(f"   Train:    Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
        print(f"   Val:      Loss={val_results['loss']:.4f}, Acc={val_results['acc']:.2f}%")
        print(f"   Val Top3: {val_results['top3_acc']:.2f}%, Top5: {val_results['top5_acc']:.2f}%")
        print(f"   Gap:      {gap:.2f}%, Time: {epoch_time:.0f}s")
        
        # Check for best model
        if val_results['acc'] > best_val_acc:
            best_val_acc = val_results['acc']
            best_top5_acc = val_results['top5_acc']
            patience_counter = 0
            
            # Save best model
            save_path = os.path.join(config.SAVE_DIR, "best_model_fast_quality.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_acc': val_results['acc'],
                'val_top3_acc': val_results['top3_acc'],
                'val_top5_acc': val_results['top5_acc'],
                'train_acc': train_acc,
                'config': {
                    'model_name': config.MODEL_NAME,
                    'num_classes': config.NUM_CLASSES,
                    'batch_size': config.BATCH_SIZE,
                    'learning_rate': config.LEARNING_RATE,
                }
            }, save_path)
            print(f"   âœ… NEW BEST! Saved to {save_path}")
        else:
            patience_counter += 1
            print(f"   â³ No improvement ({patience_counter}/{config.PATIENCE})")
        
        # Early stopping
        if patience_counter >= config.PATIENCE:
            print(f"\nâš ï¸  Early stopping triggered at epoch {epoch}")
            break
        
        # Save checkpoint every 10 epochs
        if epoch % 10 == 0:
            checkpoint_path = os.path.join(config.SAVE_DIR, f"checkpoint_epoch_{epoch}.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_results['acc'],
            }, checkpoint_path)
    
    # Training complete
    total_time = time.time() - total_start_time
    
    print("\n" + "=" * 70)
    print("ðŸŽ‰ TRAINING COMPLETE")
    print("=" * 70)
    
    print(f"\nðŸ“Š Final Results:")
    print(f"   Best Validation Accuracy: {best_val_acc:.2f}%")
    print(f"   Best Top-5 Accuracy:      {best_top5_acc:.2f}%")
    print(f"   Total Training Time:      {total_time/60:.1f} minutes")
    print(f"   Average Time per Epoch:   {total_time/epoch:.0f} seconds")
    
    # Save training history
    history_path = os.path.join(config.LOG_DIR, "training_history.json")
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    print(f"\nðŸ“ˆ Training history saved to: {history_path}")
    
    # Save final config
    config_path = os.path.join(config.LOG_DIR, "config.json")
    config_dict = {k: str(v) if not isinstance(v, (int, float, bool, str, list)) else v 
                   for k, v in vars(config).items() if not k.startswith('_')}
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=2)
    
    # Compare with original
    original_val_acc = 68.79
    improvement = best_val_acc - original_val_acc
    print(f"\nðŸ“ˆ Comparison with Original Model:")
    print(f"   Original Val Acc: {original_val_acc:.2f}%")
    print(f"   New Val Acc:      {best_val_acc:.2f}%")
    print(f"   Change:           {improvement:+.2f}%")
    
    if best_val_acc > 95:
        print(f"\nâš ï¸  Warning: >95% accuracy might indicate data issues")
    elif best_val_acc < 100:
        print(f"\nâœ… Results look realistic (no data leakage)")


if __name__ == "__main__":
    main()